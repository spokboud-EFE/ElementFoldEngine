# ElementFold Â· experience/adapters/audio.py
# Audio adapter = bridge from (model, prompt, style) â†’ JSON payload suitable for the UI.
#
# Contract with the registry:
#   factory = AdapterRegistry.get("audio")
#   runner  = factory()                                # zeroâ€‘arg â†’ callable
#   out     = runner(model, prompt, style)             # returns a dict (JSONâ€‘serializable)
#
# Behavior (plain words):
#   â€¢ Optionally apply steering controls (Î² exposure, Î³ damping, â›” clamp) from a raw â„â¸ style vector or a dict.
#   â€¢ Accept prompt as str or dict; parse generation hints (sr / seconds / len) safely.
#   â€¢ Tokenize the prompt to build a seed batch of length T (clamped to model.seq_len).
#   â€¢ Decode tokens via the shared infer_loop ('greedy' by default; 'sample' is available).
#   â€¢ Convert tokens into unsigned 8â€‘bit PCM WAV (mono) in memory and return both base64 and a data URL.
#   â€¢ Make predictions visible: include a oneâ€‘line caption summarizing Î²/Î³/â›” and decode knobs.
#
# Why 8â€‘bit PCM?  It matches the projectâ€™s byteâ€‘level tokenization in datasets/audio_folder.py:
#   training maps floats in [âˆ’1,1] â†’ uint8 [0,255]. We reverse that for simple synthesis.

from __future__ import annotations
from typing import Any, Dict, Tuple, Optional

import base64           # base64 encoding for JSON transport
import io               # inâ€‘memory WAV buffers
import re               # parse sr=, seconds=, len= from string prompts
import wave             # stdlib WAV writer

import torch

from .base import AdapterRegistry                         # ðŸ—‚ adapter registry
from elementfold.core.tokenizer import SimpleTokenizer         # âœ´ tiny byte tokenizer
from elementfold.core.infer import infer_loop                  # âœ´ unified decoding path across adapters

# Optional steering support: map raw â„â¸ â†’ {'beta','gamma','clamp','style'}; optional caption via describe()
try:
    from ..steering import SteeringController             # ðŸŽš intent â†’ control vector (and to_params)
    _HAS_STEER = True
except Exception:
    _HAS_STEER = False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Style â†’ model control (Î², Î³, â›”); accept dicts or raw vectors
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _map_style_to_params(style: Any) -> Optional[Dict[str, float]]:
    """
    Accept either:
      â€¢ dict with {'beta','gamma','clamp'} floats,
      â€¢ raw â„â¸ vector (Tensor/list/tuple) from SteeringController â†’ map via to_params(),
    and return a clean params dict or None.
    """
    if isinstance(style, dict) and all(k in style for k in ("beta", "gamma", "clamp")):
        try:
            return {
                "beta": float(style["beta"]),
                "gamma": float(style["gamma"]),
                "clamp": float(style["clamp"]),
            }
        except Exception:
            return None

    if _HAS_STEER and isinstance(style, (torch.Tensor, list, tuple)) and len(style) >= 3:
        try:
            v = torch.as_tensor(style, dtype=torch.float32)
            mapped = SteeringController.to_params(v)  # â†’ {'beta','gamma','clamp','style'}
            return {"beta": float(mapped["beta"]),
                    "gamma": float(mapped["gamma"]),
                    "clamp": float(mapped["clamp"])}
        except Exception:
            return None

    return None


def _apply_style_to_model(model, style) -> Dict[str, float]:
    """
    Determine controls from `style` and, if the model supports it, apply:
        model.apply_control(beta=?, gamma=?, clamp=?)
    Returns the dict actually applied (or {}).
    """
    params = _map_style_to_params(style)
    if params and hasattr(model, "apply_control"):
        try:
            model.apply_control(beta=params["beta"], gamma=params["gamma"], clamp=params["clamp"])
        except Exception:
            pass  # Nonâ€‘fatal: model might not implement the exact signature
    return params or {}


def _summarize_style(style: Any, params: Dict[str, float]) -> str:
    """
    Humanâ€‘friendly single line. Prefer SteeringController.describe(raw â„â¸) when available;
    otherwise fall back to the applied params. Returns "" if nothing to say.
    """
    if _HAS_STEER and isinstance(style, (torch.Tensor, list, tuple)) and len(style) >= 3:
        try:
            v = torch.as_tensor(style, dtype=torch.float32)
            # SteeringController.describe(...) is optional; fall back on failure.
            if hasattr(SteeringController, "describe"):
                return SteeringController.describe(v)  # e.g., "Î²=1.26  Î³=0.43  â›”=5.7  |  styleâ‰ˆ[...]"
        except Exception:
            pass

    if params:
        return f"Î²={params['beta']:.2f}  Î³={params['gamma']:.2f}  â›”={params['clamp']:.1f}"

    return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompt hints: "sr=16000", "seconds=1.0", "len=16000" in str or dict
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_HINT_RE = re.compile(
    r"(sr|sample_rate|seconds|sec|len|length)\s*=\s*([0-9]+(?:\.[0-9]+)?)",
    re.IGNORECASE,
)

def _parse_prompt_hints(s_or_dict: Any, default_sr: int, default_len: int) -> Tuple[int, int]:
    """
    Extract simple generation hints from either a string prompt *or* a dict prompt:
      â€¢ sr / sample_rate = integer Hz (e.g., 16000)
      â€¢ seconds / sec    = float seconds â†’ length = sr * seconds
      â€¢ len / length     = integer number of samples (tokens)
    Returns (sr, length_tokens) clamped to safe ranges.
    """
    sr = int(default_sr)
    length = int(default_len)

    # Dict path (structured)
    if isinstance(s_or_dict, dict):
        def _num(x):
            try:
                return None if x is None else float(x)
            except Exception:
                return None

        sr_from_dict = _num(s_or_dict.get("sr")) or _num(s_or_dict.get("sample_rate"))
        if sr_from_dict is not None:
            sr = int(sr_from_dict)

        seconds_from_dict = _num(s_or_dict.get("seconds")) or _num(s_or_dict.get("sec"))
        if seconds_from_dict is not None:
            length = int(max(1, round(sr * float(seconds_from_dict))))

        len_from_dict = _num(s_or_dict.get("len")) or _num(s_or_dict.get("length"))
        if len_from_dict is not None:
            length = int(len_from_dict)

    # String path (backâ€‘compat / quick experiments)
    elif isinstance(s_or_dict, str):
        for key, val in _HINT_RE.findall(s_or_dict):
            key_l = key.lower()
            if key_l in ("sr", "sample_rate"):
                try:
                    sr = int(float(val))
                except Exception:
                    pass
            elif key_l in ("seconds", "sec"):
                try:
                    length = int(max(1, round(sr * float(val))))
                except Exception:
                    pass
            elif key_l in ("len", "length"):
                try:
                    length = int(float(val))
                except Exception:
                    pass

    # Rails: keep memory stable and WAV sane
    sr = int(max(8000, min(48000, sr)))          # 8 kHz â€¦ 48 kHz
    length = int(max(1, min(4 * sr, length)))    # up to ~4 seconds by default
    return sr, length


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tokens â†’ WAV (unsigned 8â€‘bit mono), Base64 for transport
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _tokens_to_wav_b64(tokens: torch.Tensor, sample_rate: int) -> Tuple[str, float]:
    """
    Take a 1â€‘D LongTensor (values 0..255), write an unsigned 8â€‘bit mono WAV into memory,
    and return (base64_string, duration_sec).
    """
    q = tokens.to(torch.uint8).contiguous().view(-1).cpu()   # bytes in [0,255]
    duration = float(q.numel()) / float(sample_rate)

    bio = io.BytesIO()
    with wave.open(bio, "wb") as w:
        w.setnchannels(1)               # mono
        w.setsampwidth(1)               # 1 byte = 8â€‘bit unsigned PCM
        w.setframerate(int(sample_rate))
        w.writeframes(q.numpy().tobytes())

    wav_b64 = base64.b64encode(bio.getvalue()).decode("ascii")
    return wav_b64, duration


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core adapter runner (unified with infer_loop)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _run(model, prompt, style):
    """
    1) Optionally apply steering controls to the model (Î², Î³, â›”).
    2) Parse prompt hints (sr=â€¦, seconds=â€¦, len=â€¦) from str or dict.
    3) Build an input token batch (seed) of requested length T, clamped to model.seq_len.
    4) Decode via infer_loop (strategy='greedy' unless overridden in prompt dict['decode']).
    5) Pack tokens into a base64 WAV and return a JSONâ€‘friendly dict with a human caption.
    """
    # 1) Steering (noâ€‘op if style is None or model lacks apply_control)
    applied = _apply_style_to_model(model, style)
    caption = _summarize_style(style, applied)

    # 2) Determine target length and sample rate from prompt hints
    T_default = int(getattr(model, "seq_len", 128))  # modelâ€™s configured max length
    sr, T_req = _parse_prompt_hints(prompt, default_sr=16000, default_len=T_default)
    T = int(min(T_req, T_default))                   # always respect modelâ€™s max

    # 3) Prepare seed token batch from prompt text (for dict: use prompt.get("text", ""))
    tok = SimpleTokenizer()
    if isinstance(prompt, dict):
        text_seed = str(prompt.get("text", ""))
    else:
        text_seed = str(prompt or "")
    ids = tok.encode(text_seed) or [0]               # ensure at least one id

    # Device and batch pack
    try:
        dev = next(model.parameters()).device
    except Exception:
        dev = torch.device("cpu")
    x = torch.tensor(ids[:T], dtype=torch.long, device=dev).unsqueeze(0)  # (1,â‰¤T)

    # Pad to T if needed (rightâ€‘pad zeros â†’ neutral byte)
    if x.size(1) < T:
        pad = torch.zeros(1, T - x.size(1), dtype=torch.long, device=dev)
        x = torch.cat([x, pad], dim=1)

    # 4) Decode via the shared inference path for consistency
    # Allow dict prompts to pass decoding knobs: {'decode': {'strategy': 'sample', 'temperature': 0.8, ...}}
    strategy = "greedy"
    temperature = 1.0
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    if isinstance(prompt, dict):
        dec = prompt.get("decode", {})
        if isinstance(dec, dict):
            strategy = str(dec.get("strategy", strategy))
            try:
                temperature = float(dec.get("temperature", temperature))
            except Exception:
                pass
            try:
                tk = dec.get("top_k", top_k)
                top_k = int(tk) if tk is not None else None
            except Exception:
                top_k = None
            try:
                tp = dec.get("top_p", top_p)
                tp = float(tp) if tp is not None else None
                top_p = tp if (tp is None or 0.0 < tp < 1.0) else None
            except Exception:
                top_p = None

    out = infer_loop(
        model,
        x=x,
        strategy=strategy,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
    )
    y = out["tokens"].squeeze(0)                 # (T,) int64 in [0..Vâˆ’1]

    # Map decoded ids into 0..255 for audio (wrap if vocab>256; expand range if vocab<256)
    q = (y.to(torch.int64) % 256).to(torch.uint8)

    # 5) Pack into WAV and return payload
    wav_b64, duration = _tokens_to_wav_b64(q, sample_rate=sr)
    return {
        "wav_b64": wav_b64,                              # base64 WAV (mono, 8â€‘bit)
        "data_url": f"data:audio/wav;base64,{wav_b64}",  # handy for browsers
        "tokens": q.tolist(),                            # decoded token sequence (ints 0..255)
        "sr": int(sr),                                   # sample rate
        "duration_sec": float(duration),                 # seconds
        "applied": applied,                              # {'beta','gamma','clamp'} if steering was applied
        "decode": {                                      # echo knobs for traceability
            "strategy": strategy,
            "temperature": float(temperature),
            "top_k": (int(top_k) if top_k is not None else None),
            "top_p": (float(top_p) if top_p is not None else None),
        },
        "caption": caption,                              # humanâ€‘readable summary (Î²/Î³/â›”, style sketch)
    }


# â€” Registry wiring: decorator form keeps registration concise and consistent â€”
@AdapterRegistry.register_fn("audio")
def make_audio_adapter():
    # Zeroâ€‘arg factory â†’ runner(model, prompt, style) â†’ dict
    return _run
