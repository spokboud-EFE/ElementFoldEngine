# ElementFold Â· experience/adapters/language.py
# Language adapter = tiny bridge from (model, prompt, style) â†’ text.
# Contract with the registry:
#   factory = AdapterRegistry.get("language")
#   runner  = factory()                          # zeroâ€‘arg â†’ callable
#   out     = runner(model, prompt, style)       # returns a string
#
# Behavior:
#   â€¢ Tokenize the prompt with SimpleTokenizer (vocabâ‰ˆ256).
#   â€¢ Optionally apply steering controls (Î², Î³, â›”) to the model if a raw style vector is provided.
#   â€¢ Run a single forward pass and greedyâ€‘decode logits to tokens.
#   â€¢ Detokenize tokens back to text.
#
#   The â€œstyleâ€ input may be the raw â„â¸ vector produced by the SteeringController:
#     v = [Î²Ì‚, Î³Ì‚, â›”Ì‚, styleâ‚…]
#   We map it into meaningful ranges and apply (Î², Î³, â›”) to the modelâ€™s Foldâ€“Gateâ€“Norm blocks.
#   Adapters are intentionally small so theyâ€™re easy to reason about and replace.

import torch                                    # âœ´ tensors
from .base import AdapterRegistry               # ðŸ—‚ adapter registry
from ...tokenizer import SimpleTokenizer        # âœ´ tokenizer
try:
    # Optional import: used only to map raw style vectors into (beta, gamma, clamp).
    from ..steering import SteeringController   # ðŸŽš intent â†’ control vector (and to_params mapping)
    _HAS_STEER = True
except Exception:
    _HAS_STEER = False


def _apply_style_to_model(model, style):
    """
    If `style` looks like a raw â„â¸ vector from SteeringController, map it to parameters
    and apply to the model (if it implements .apply_control). If `style` is already a dict
    with beta/gamma/clamp, use it directly. Otherwise, do nothing.

    Returns a dict of the parameters that were (or would be) applied.
    """
    params = None

    # case A: dict with explicit params
    if isinstance(style, dict) and all(k in style for k in ("beta", "gamma", "clamp")):
        params = {"beta": float(style["beta"]), "gamma": float(style["gamma"]), "clamp": float(style["clamp"])}

    # case B: Tensor / list that looks like â„â¸ from SteeringController
    elif _HAS_STEER and isinstance(style, (torch.Tensor, list, tuple)) and len(style) >= 3:
        v = torch.as_tensor(style, dtype=torch.float32)         # normalize type
        params = SteeringController.to_params(v)                # map raw â†’ ranges

    # Apply if supported
    if params and hasattr(model, "apply_control"):
        model.apply_control(beta=params["beta"], gamma=params["gamma"], clamp=params["clamp"])

    return params or {}


def _run(model, prompt, style):
    """
    Core language adapter runner:
      1) optional steering â†’ apply (Î², Î³, â›”),
      2) tokenize prompt,
      3) forward once,
      4) greedy decode,
      5) detokenize to string.
    """
    # 1) Optionally apply steering controls to the model (noâ€‘op if not provided/unsupported).
    _apply_style_to_model(model, style)

    # 2) Tokenize the prompt; ensure at least one token for empty strings.
    tok = SimpleTokenizer()                                   # âœ´ tokenizer instance
    ids = tok.encode(prompt or "")                            # â†¦ token ids (list[int])
    if len(ids) == 0:
        ids = [0]                                             # neutral token if prompt is empty

    # 3) Build a batch tensor and clip to the model's sequence length.
    dev = next(model.parameters()).device                     # ðŸ–¥ model device
    T = int(getattr(model, "seq_len", len(ids)))              # max tokens the model expects
    x = torch.tensor(ids[:T], dtype=torch.long, device=dev).unsqueeze(0)  # (1,T')

    # 4) Forward pass in noâ€‘grad mode; decode greedily.
    with torch.no_grad():                                     # â‰¡ eval path
        logits, _X = model(x)                                 # âŸ² forward â†’ (1,T',V),(1,T')
        y = logits.argmax(dim=-1).squeeze(0).tolist()         # greedy decode ids

    # 5) Detokenize to a humanâ€‘readable string and return.
    return tok.decode(y)                                      # â†¤ text


# â€” registry wiring: provide a zeroâ€‘arg factory that returns the runner â€”
AdapterRegistry.register("language", lambda: _runner)

def _runner(model, prompt, style):
    return _run(model, prompt, style)
